name: analyze-equilibria

on:
  workflow_dispatch:
    inputs:
      learn-run-id:
        description: 'The run ID for the Learn job'
        required: true
        default: ''

jobs:
  analyze:
    runs-on: [self-hosted]
    timeout-minutes: 3600

    steps:
      - uses: actions/checkout@v2

      # Docker creates files as owner root (0). That means when the
      # runner, running as github, tries to delete the artifacts, it
      # fails. So we create a fresh copy of the current directory
      # under ~/run/$job_id and then remove that later with sudo. The
      # github user is a sudoer. That's not ideal, but the self-hosted
      # runner is private anyway.
      - name: Run analysis process
        env:
          LEARN_RUNID: ${{ github.event.inputs.learn-run-id }}
        run: |
          echo Workflow was run with input: learn-run-id=$LEARN_RUNID

          # Grab experiment outputs
          LEARN_FILENAME=$LEARN_RUNID-equilibrium_phase1.tar.gz
          ARCHIVE_LEARN=$(pwd)/$LEARN_FILENAME

          # Setup directories
          mkdir experiment
          mkdir outputs

          # Download & unpack learning output
          mkdir extraction
          time s3cmd get s3://pricing-game/$LEARN_FILENAME $ARCHIVE_LEARN
          cd extraction
          tar xzf $ARCHIVE_LEARN
          cd ..
          mv extraction/* experiment/
          rm -r extraction
          # End of experiment extraction

          # Setup working directory
          ARCHIVE_ANALYSIS=$(pwd)/$GITHUB_RUN_ID-analysis.tar.gz
          WORKDIR=~/run/$GITHUB_RUN_ID
          sudo mkdir -p $WORKDIR $JOBDIR
          sudo cp -r . $WORKDIR/

          # Run job
          time docker run --privileged \
             -v$WORKDIR:$WORKDIR -w$WORKDIR \
             -v$WORKDIR/experiment:/experiment \
             -v$WORKDIR/outputs:/outputs \
             --rm \
             ghcr.io/learning-games/haskellbase:2022-02-17 \
             sh -c "stack build -j1 --exec 'eq-analysis'"

          # Archive output
          PW=$(pwd)
          cd $WORKDIR/outputs
          time tar czf $ARCHIVE_ANALYSIS .
          cd $PW

          # Upload output
          time s3cmd put $ARCHIVE_ANALYSIS s3://pricing-game/

          # Wipe working directory
          sudo rm -rf $WORKDIR 
