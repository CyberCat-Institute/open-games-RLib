name: Learn then Analyze

on:
  workflow_dispatch:
    inputs:
      logLevel:
        description: 'Do not change'
        required: true
        default: 'dummy input'
      tags:
        description: 'dummy input, ignore'

jobs:
  learn:
    runs-on: self-hosted-ubuntu-with-docker
    timeout-minutes: 1440

    steps:
      - uses: actions/checkout@v2

      - name: Try to login to docker hub registry
        run: |
          docker logout ghcr.io
          echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      # Docker creates files as owner root (0). That means when the
      # runner, running as github, tries to delete the artifacts, it
      # fails. So we create a fresh copy of the current directory
      # under ~/run/$job_id and then remove that later with sudo. The
      # github user is a sudoer. That's not ideal, but the self-hosted
      # runner is private anyway.
      - name: Run learning process
        run: |
          # Setup working directory
          LEARN_FILENAME=$GITHUB_RUN_ID-learn.tar.gz
          ARCHIVE_LEARN=$(pwd)/$LEARN_FILENAME
          WORKDIR=~/run/$GITHUB_RUN_ID
          JOBDIR=$WORKDIR/results
          sudo mkdir -p $WORKDIR $JOBDIR
          sudo cp -r . $WORKDIR/

          # Run job
          # NOTE Update information on game here
          time docker run -v$WORKDIR:$WORKDIR -w$WORKDIR --rm ghcr.io/learning-games/base:2021-11-01 sh -c "stack build -j1 --fast --exec 'game local asymmetricLearners3PhasesRobustnessExplorationConvergence --skip-git'"

          # Archive output
          PW=$(pwd)
          cd $JOBDIR
          time tar czf $ARCHIVE_LEARN .
          cd $PW
          sudo rm -rf $JOBDIR

          # Upload output
          time s3cmd put $ARCHIVE_LEARN s3://pricing-game/

          # Wipe working directory
          sudo rm -rf $WORKDIR
  analyze:
    needs: learn

    runs-on: self-hosted-ubuntu-with-docker

    steps:
      - uses: actions/checkout@v2

      - name: Try to login to docker hub registry
        run: echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      # Remember to update this hash whenever you update your R dependencies in any way
      - name: Pull down image
        run: |
          # NOTE: Update R image here
          time docker pull ghcr.io/learning-games/r:2021-11-28@sha256:46bd2520e4578407160f4de50d1dc8c0ace7577777968520945e7ac3351606a0

      # Docker creates files as owner root (0). That means when the
      # runner, running as github, tries to delete the artifacts, it
      # fails. So we create a fresh copy of the current directory
      # under ~/run/$job_id and then remove that later with sudo. The
      # github user is a sudoer. That's not ideal, but the self-hosted
      # runner is private anyway.
      - name: Run analysis process
        run: |
          # Grab experiment outputs
          LEARN_FILENAME=$GITHUB_RUN_ID-learn.tar.gz
          ARCHIVE_LEARN=$(pwd)/$LEARN_FILENAME

          # Setup directories
          mkdir experiment
          mkdir outputs

          # Download & unpack learning output
          mkdir extraction
          time s3cmd get s3://pricing-game/$LEARN_FILENAME $ARCHIVE_LEARN
          cd extraction
          tar xzf $ARCHIVE_LEARN
          cd ..
          mv extraction/*/* experiment/
          rm -r extraction
          # End of experiment extraction

          # Setup working directory
          ARCHIVE_ANALYSIS=$(pwd)/$GITHUB_RUN_ID-analysis.tar.gz
          WORKDIR=~/run/$GITHUB_RUN_ID
          sudo mkdir -p $WORKDIR $JOBDIR
          sudo cp -r . $WORKDIR/

          # Run job
          docker run \
            -v$WORKDIR/outputs:/outputs \
            -v$WORKDIR/Rscripts:/Rscripts \
            -v$WORKDIR/experiment:/experiment \
            --rm \
            # NOTE update image here
            ghcr.io/learning-games/r:2021-11-28 \
            # NOTE Update R file here
            R -f /Rscripts/asymlearners_robustness.R

          echo Behold, the outputs are...
          ls -alh $WORKDIR/outputs

          # Archive output
          PW=$(pwd)
          cd $WORKDIR/outputs
          time tar czf $ARCHIVE_ANALYSIS .
          cd $PW

          # Upload output
          time s3cmd put $ARCHIVE_ANALYSIS s3://pricing-game/

          # Wipe working directory
          sudo rm -rf $WORKDIR
