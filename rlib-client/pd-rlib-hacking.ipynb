{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239fd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "import numpy as np\n",
    "import copy\n",
    "import requests\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(local_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f859ac",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- This is a two-player game. So, maybe we could fix one player to always do the same thing, and then learn against that?\n",
    "- Or, we could learn two agents?\n",
    "- Or ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IteratedPDEnv(gym.Env):\n",
    "    \n",
    "    done  = False\n",
    "    lastPlayer1Action = 0 # Cooperate\n",
    "    \n",
    "    def __init__ (self, env_config):\n",
    "        # Note: Our action space is for ONE player; namely, exactly two\n",
    "        # choices:\n",
    "        #  - Defect\n",
    "        #  - Cooperate\n",
    "        self.action_space      = gym.spaces.Discrete(2)\n",
    "        \n",
    "        # We make no observations.\n",
    "        self.observation_space = gym.spaces.Tuple((gym.spaces.Discrete(2), gym.spaces.Discrete(2)))\n",
    "\n",
    "        # self.seed(1)\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset (self):\n",
    "        self.done = False\n",
    "        # print (\"Reset was called!\")\n",
    "        return (0, 0)\n",
    "    \n",
    "    def action_to_int(self, action):\n",
    "        if action == \"Cooperate\":\n",
    "            return 0\n",
    "        elif action == \"Defect\":\n",
    "            return 1\n",
    "\n",
    "    def int_to_action(self, i):\n",
    "        if i == 0:\n",
    "            return \"Cooperate\"\n",
    "        elif i == 1:\n",
    "            return \"Defect\"\n",
    "    \n",
    "    def step (self, action):\n",
    "        # action is either 0 or 1.\n",
    "        assert action in [0, 1], \"Unknown action!\"\n",
    "        \n",
    "        player1Action = self.int_to_action(action)\n",
    "        \n",
    "        player2Action = \"Cooperate\"                                 # Goody-two-shoes\n",
    "        player2Action = \"Defect\"                                    # Betrayal ...\n",
    "        player2Action = self.int_to_action(self.lastPlayer1Action)  # Copycat! (tit-for-tat)\n",
    "        \n",
    "        # We are done once \"step\" is called; a round of the game is a single episode.\n",
    "        self.done = True\n",
    "        \n",
    "        # Register the current move of the player so that the opponent can copy it in the next round\n",
    "        self.lastPlayer1Action = action\n",
    "        \n",
    "        data = { \"player1Action\": player1Action\n",
    "               , \"player2Action\": player2Action\n",
    "               }\n",
    "        \n",
    "        # Do a post to the server; get the payoffs.\n",
    "        response = requests.post(\"http://localhost:3000/play\", json=data).json()\n",
    "        \n",
    "        reward = response[\"player1Payoff\"]\n",
    "        \n",
    "        obs = (self.action_to_int (player1Action), self.action_to_int (player2Action))\n",
    "        \n",
    "        info = { \"response\": response,\n",
    "                \"lastPlayer1Action\": self.lastPlayer1Action }\n",
    "        \n",
    "        return [ obs, reward, self.done, info ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = IteratedPDEnv(env_config = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ddbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(action=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0fe49",
   "metadata": {},
   "source": [
    "### Let's try training it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(env=IteratedPDEnv, config={\n",
    "    \"framework\": \"tf2\",\n",
    "    \"num_workers\": 1,\n",
    "    \"env_config\": {},\n",
    "    \"create_env_on_driver\": True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800948a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Training loop {i}\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16259689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, open TensorBoard:\n",
    "# cd ~/ray_results && conda activate rlib-client && tensorboard --logdir ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
