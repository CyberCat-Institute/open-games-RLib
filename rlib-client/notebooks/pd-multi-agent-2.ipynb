{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239fd0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "import numpy as np\n",
    "import copy\n",
    "import requests\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfb0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(local_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b98ad7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "COOPERATE = 0\n",
    "DEFECT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf74288d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IteratedPDEnv(MultiAgentEnv):\n",
    "    def __init__ (self, env_config={}):\n",
    "        # Exactly two agents.\n",
    "        self.num_agents = 2\n",
    "        self._agent_ids = [f\"agent-{i}\" for i in range(self.num_agents)]\n",
    "        \n",
    "        # Note: Our action space is for ONE player; namely, exactly two\n",
    "        # choices:IteratedPDEnv\n",
    "        #  - Defect\n",
    "        #  - Cooperate\n",
    "        self.action_space      = gym.spaces.Discrete(2)\n",
    "        \n",
    "        # Our observation is simply the last move of both players.\n",
    "        self.observation_space = gym.spaces.Tuple((gym.spaces.Discrete(2), gym.spaces.Discrete(2)))\n",
    "    \n",
    "    \n",
    "    def reset (self):  \n",
    "        obs = {}\n",
    "        for i in range(self.num_agents):\n",
    "            obs[i] = (COOPERATE, COOPERATE)\n",
    "        return obs\n",
    "    \n",
    "    def int_to_action(self, i):\n",
    "        if i == 0:\n",
    "            return \"Cooperate\"\n",
    "        elif i == 1:\n",
    "            return \"Defect\"\n",
    "    \n",
    "    def step (self, action_dict):        \n",
    "        playerAction = {}\n",
    "        print(action_dict)\n",
    "        for i in range(self.num_agents):\n",
    "            # action is either 0 or 1.\n",
    "            assert action_dict[i] in [0, 1], \"Unknown action!\"\n",
    "            playerAction[i] = self.int_to_action(action_dict[i])\n",
    "\n",
    "        data = { \"player1Action\": playerAction[0]\n",
    "               , \"player2Action\": playerAction[1]\n",
    "               }\n",
    "        \n",
    "        # Do a post to the server; get the payoffs.\n",
    "        response = requests.post(\"http://localhost:3000/play\", json=data).json()\n",
    "\n",
    "        reward = {}\n",
    "        for i in range(self.num_agents):\n",
    "            reward[i] = response[f\"player{i+1}Payoff\"]\n",
    "\n",
    "        obs = copy.copy(action_dict)\n",
    "        rew = reward\n",
    "        \n",
    "        dones = {}\n",
    "        infos = {}\n",
    "        for i in range(self.num_agents):\n",
    "            dones[i] = True\n",
    "            infos[i] = {}\n",
    "\n",
    "        dones[\"__all__\"] = True\n",
    "        \n",
    "        return obs, rew, dones, infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d1a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune import register_env\n",
    "\n",
    "def env_creator(_):\n",
    "    return IteratedPDEnv()\n",
    "single_env = IteratedPDEnv()\n",
    "env_name = \"IteratedPDEnv\"\n",
    "register_env(env_name, env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e8d7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "def gen_policy():\n",
    "    return (None, obs_space, act_space, {})\n",
    "policy_graphs = {}\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy()\n",
    "def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1512f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "    \"num_workers\": 3,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"lr\": 5e-3,\n",
    "    \"model\":{\"fcnet_hiddens\": [8, 8]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": \"IteratedPDEnv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584587b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "\n",
    "exp_name = 'more_jail_time_yay'\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'PG',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 100\n",
    "        },\n",
    "        'checkpoint_freq': 20,\n",
    "        \"config\": config,\n",
    "}\n",
    "# ray.init()\n",
    "tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f6d324",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = IteratedPDEnv(env_config = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ddbb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(action=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e0fe49",
   "metadata": {},
   "source": [
    "### Let's try training it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21f81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(env=IteratedPDEnv, config={\n",
    "    \"framework\": \"tf2\",\n",
    "    \"num_workers\": 1,\n",
    "    \"env_config\": {},\n",
    "    \"create_env_on_driver\": True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800948a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"Training loop {i}\")\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc9671a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16259689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, open TensorBoard:\n",
    "# cd ~/ray_results && conda activate rlib-client && tensorboard --logdir ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
