{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab43f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef277193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a58d5c",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- This is a two-player game. So, maybe we could fix one player to always do the same thing, and then learn against that?\n",
    "- Or, we could learn two agents?\n",
    "- Or ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6deebf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "72d74fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IteratedPDEnv(gym.Env):\n",
    "    \n",
    "    INITIAL_STATE = {\"totalPlayer2\": 0, \"totalPlayer1\": 0}\n",
    "    \n",
    "    state = None\n",
    "    \n",
    "    def __init__ (self, env_config):\n",
    "        # Note: Our action space is for ONE player; namely, exactly two\n",
    "        # choices:\n",
    "        #  - Defect\n",
    "        #  - Cooperate\n",
    "        self.action_space      = gym.spaces.Discrete(2)\n",
    "        \n",
    "        # Our observation is simply the total jail time for ourselves (Player 1)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                                      low=0\n",
    "                                    , high=np.iinfo(np.int32).max\n",
    "                                    , shape=(1,)\n",
    "                                    , dtype=np.int32\n",
    "                                    )\n",
    "        # self.seed(1)\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset (self):\n",
    "        self.state = copy.deepcopy(self.INITIAL_STATE)\n",
    "    \n",
    "    \n",
    "    def step (self, action):\n",
    "        # action is either 0 or 1.\n",
    "        \n",
    "        if action == 0:\n",
    "            player1Action = \"Cooperate\"\n",
    "        else:\n",
    "            player1Action = \"Defect\"\n",
    "        \n",
    "        assert action in [0, 1], \"Unknown action!\"\n",
    "        \n",
    "        data = { \"player1Action\": player1Action\n",
    "               , \"player2Action\": \"Cooperate\" # Goody-two-shoes\n",
    "               , \"startingState\": self.state\n",
    "               }\n",
    "        \n",
    "        # Do a post to the server; get the payoffs.\n",
    "        response = requests.post(\"http://localhost:3000/play\", json=data).json()\n",
    "        \n",
    "        print(response)\n",
    "        \n",
    "        # Update the state.\n",
    "        self.state[\"totalPlayer1\"] += response[\"lastPlayer1Payoff\"]\n",
    "        self.state[\"totalPlayer2\"] += response[\"lastPlayer2Payoff\"]\n",
    "        \n",
    "        return [self.state, self.state[\"totalPlayer1\"], False, {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa451d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = IteratedPDEnv(env_config = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b5c050b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lastPlayer1Payoff': 5, 'lastPlayer2Payoff': 0, 'gameState': {'totalPlayer2': 0, 'totalPlayer1': 5}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'totalPlayer2': 0, 'totalPlayer1': 5}, 5, False, {}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0f4b8",
   "metadata": {},
   "source": [
    "### Let's try training it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5635edc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.212',\n",
       " 'raylet_ip_address': '192.168.1.212',\n",
       " 'redis_address': None,\n",
       " 'object_store_address': '/tmp/ray/session_2022-03-30_10-47-56_980375_3715266/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2022-03-30_10-47-56_980375_3715266/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2022-03-30_10-47-56_980375_3715266',\n",
       " 'metrics_export_port': 65133,\n",
       " 'gcs_address': '192.168.1.212:62235',\n",
       " 'address': '192.168.1.212:62235',\n",
       " 'node_id': '3ac284d8e86770557b91570f77372cc72de49efdd743b70195bebca7'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1611c6bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715394)\u001b[0m 2022-03-30 10:54:32,781\tERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3715394, ip=192.168.1.212, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7fb4a4108c40>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715394)\u001b[0m   File \"/home/noon/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 462, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715394)\u001b[0m     _validate_env(self.env, env_context=self.env_context)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715394)\u001b[0m   File \"/home/noon/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1718, in _validate_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715394)\u001b[0m     env.observation_space.dtype != dummy_obs.dtype:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715394)\u001b[0m AttributeError: 'NoneType' object has no attribute 'dtype'\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715395)\u001b[0m 2022-03-30 10:54:32,744\tERROR worker.py:430 -- Exception raised in creation task: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3715395, ip=192.168.1.212, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4ded8e9be0>)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715395)\u001b[0m   File \"/home/noon/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 462, in __init__\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715395)\u001b[0m     _validate_env(self.env, env_context=self.env_context)\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715395)\u001b[0m   File \"/home/noon/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1718, in _validate_env\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715395)\u001b[0m     env.observation_space.dtype != dummy_obs.dtype:\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=3715395)\u001b[0m AttributeError: 'NoneType' object has no attribute 'dtype'\n"
     ]
    },
    {
     "ename": "RayActorError",
     "evalue": "The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3715395, ip=192.168.1.212, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4ded8e9be0>)\n  File \"/home/noon/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 462, in __init__\n    _validate_env(self.env, env_context=self.env_context)\n  File \"/home/noon/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1718, in _validate_env\n    env.observation_space.dtype != dummy_obs.dtype:\nAttributeError: 'NoneType' object has no attribute 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:807\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 807\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# parallel to training.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n",
      "File \u001b[0;32m~/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:925\u001b[0m, in \u001b[0;36mTrainer._init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init\u001b[39m(\u001b[38;5;28mself\u001b[39m, config: TrainerConfigDict,\n\u001b[1;32m    924\u001b[0m           env_creator: EnvCreator) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 925\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRayActorError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mPPOTrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mIteratedPDEnv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mIteratedPDEnv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:746\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, config, env, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;66;03m# Initialize common evaluation_metrics to nan, before they become\u001b[39;00m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;66;03m# available. We want to make sure the metrics are always present\u001b[39;00m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# (although their values may be nan), so that Tune does not complain\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# when we use these as stopping criteria.\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_metrics \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m    740\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode_reward_max\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mnan,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    743\u001b[0m     }\n\u001b[1;32m    744\u001b[0m }\n\u001b[0;32m--> 746\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger_creator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremote_checkpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m                 \u001b[49m\u001b[43msync_function_tpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/tune/trainable.py:124\u001b[0m, in \u001b[0;36mTrainable.__init__\u001b[0;34m(self, config, logger_creator, remote_checkpoint_dir, sync_function_tpl)\u001b[0m\n\u001b[1;32m    122\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_ip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_current_ip()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m setup_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m setup_time \u001b[38;5;241m>\u001b[39m SETUP_TIME_THRESHOLD:\n",
      "File \u001b[0;32m~/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:822\u001b[0m, in \u001b[0;36mTrainer.setup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# New design: Override `Trainable.setup()` (as indented by Trainable)\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;66;03m# and do or don't call super().setup() from within your override.\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;66;03m# By default, `super().setup()` will create both worker sets:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[38;5;66;03m# parallel to training.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;66;03m# TODO: Deprecate `_init()` and remove this try/except block.\u001b[39;00m\n\u001b[1;32m    815\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     \u001b[38;5;66;03m# Only if user did not override `_init()`:\u001b[39;00m\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;66;03m# - Create rollout workers here automatically.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;66;03m# This matches the behavior of using `build_trainer()`, which\u001b[39;00m\n\u001b[1;32m    821\u001b[0m     \u001b[38;5;66;03m# should no longer be used.\u001b[39;00m\n\u001b[0;32m--> 822\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_default_policy_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;66;03m# Function defining one single training iteration's behavior.\u001b[39;00m\n\u001b[1;32m    830\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_disable_execution_plan_api\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    831\u001b[0m         \u001b[38;5;66;03m# Ensure remote workers are initially in sync with the\u001b[39;00m\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;66;03m# local worker.\u001b[39;00m\n",
      "File \u001b[0;32m~/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/agents/trainer.py:1995\u001b[0m, in \u001b[0;36mTrainer._make_workers\u001b[0;34m(self, env_creator, validate_env, policy_class, config, num_workers, local_worker)\u001b[0m\n\u001b[1;32m   1959\u001b[0m \u001b[38;5;129m@DeveloperAPI\u001b[39m\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_workers\u001b[39m(\n\u001b[1;32m   1961\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1968\u001b[0m         local_worker: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1969\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m WorkerSet:\n\u001b[1;32m   1970\u001b[0m     \u001b[38;5;124;03m\"\"\"Default factory method for a WorkerSet running under this Trainer.\u001b[39;00m\n\u001b[1;32m   1971\u001b[0m \n\u001b[1;32m   1972\u001b[0m \u001b[38;5;124;03m    Override this method by passing a custom `make_workers` into\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[38;5;124;03m        The created WorkerSet.\u001b[39;00m\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1995\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mWorkerSet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv_creator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_creator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpolicy_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1999\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainer_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2000\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2001\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_worker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2002\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogdir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogdir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2003\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py:101\u001b[0m, in \u001b[0;36mWorkerSet.__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, local_worker, logdir, _setup)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# Create a local worker, if needed.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# If num_workers > 0 and we don't have an env on the local worker,\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# get the observation- and action spaces for each policy from\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# the first remote worker (which does have an env).\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_worker \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_remote_workers \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m trainer_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_env_on_driver\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m     99\u001b[0m         (\u001b[38;5;129;01mnot\u001b[39;00m trainer_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobservation_space\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    100\u001b[0m          \u001b[38;5;129;01mnot\u001b[39;00m trainer_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maction_space\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m--> 101\u001b[0m     remote_spaces \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote_workers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforeach_policy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremote\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpid\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     spaces \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    105\u001b[0m         e[\u001b[38;5;241m0\u001b[39m]: (\u001b[38;5;28mgetattr\u001b[39m(e[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal_space\u001b[39m\u001b[38;5;124m\"\u001b[39m, e[\u001b[38;5;241m1\u001b[39m]), e[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m remote_spaces\n\u001b[1;32m    107\u001b[0m     }\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# Try to add the actual env's obs/action spaces.\u001b[39;00m\n",
      "File \u001b[0;32m~/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/_private/client_mode_hook.py:105\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/worker.py:1765\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1763\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mas_instanceof_cause()\n\u001b[1;32m   1764\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1765\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m value\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_individual_id:\n\u001b[1;32m   1768\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mRayActorError\u001b[0m: The actor died because of an error raised in its creation task, \u001b[36mray::RolloutWorker.__init__()\u001b[39m (pid=3715395, ip=192.168.1.212, repr=<ray.rllib.evaluation.rollout_worker.RolloutWorker object at 0x7f4ded8e9be0>)\n  File \"/home/noon/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 462, in __init__\n    _validate_env(self.env, env_context=self.env_context)\n  File \"/home/noon/tools/miniconda3/envs/rlib-client/lib/python3.8/site-packages/ray/rllib/evaluation/rollout_worker.py\", line 1718, in _validate_env\n    env.observation_space.dtype != dummy_obs.dtype:\nAttributeError: 'NoneType' object has no attribute 'dtype'"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(env=IteratedPDEnv, config={ \"env\": IteratedPDEnv, \"env_config\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e7705",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
