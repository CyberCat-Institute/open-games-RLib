{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b69eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "import numpy as np\n",
    "import copy\n",
    "import requests\n",
    "from ray.rllib.agents.ppo import PPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d884a49e",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "- This is a two-player game. So, maybe we could fix one player to always do the same thing, and then learn against that?\n",
    "- Or, we could learn two agents?\n",
    "- Or ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341ee4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IteratedPDEnv(gym.Env):\n",
    "    \n",
    "    INITIAL_STATE = {\"totalPlayer2\": 0, \"totalPlayer1\": 0}\n",
    "    \n",
    "    state = None\n",
    "    done =  False\n",
    "    \n",
    "    def __init__ (self, env_config):\n",
    "        # Note: Our action space is for ONE player; namely, exactly two\n",
    "        # choices:\n",
    "        #  - Defect\n",
    "        #  - Cooperate\n",
    "        self.action_space      = gym.spaces.Discrete(2)\n",
    "        \n",
    "        # We make no observations.\n",
    "        self.observation_space = gym.spaces.Discrete(1)\n",
    "        \n",
    "        \"\"\"\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "                                      low=0\n",
    "                                    , high=np.iinfo(np.int32).max\n",
    "                                    , shape=(1,)\n",
    "                                    , dtype=np.int32\n",
    "                                    )\n",
    "        \"\"\"\n",
    "        \n",
    "        # self.seed(1)\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset (self):\n",
    "        self.state = copy.deepcopy(self.INITIAL_STATE)\n",
    "        self.done = False\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "    def step (self, action):\n",
    "        # action is either 0 or 1.\n",
    "        \n",
    "        if action == 0:\n",
    "            player1Action = \"Cooperate\"\n",
    "        else:\n",
    "            player1Action = \"Defect\"\n",
    "        \n",
    "        assert action in [0, 1], \"Unknown action!\"\n",
    "        \n",
    "        # We are done if someone spent >100 years in jail.\n",
    "        if self.state[\"totalPlayer1\"] >= 100:\n",
    "            self.done = True\n",
    "            \n",
    "        if self.state[\"totalPlayer2\"] >= 100:\n",
    "            self.done = True\n",
    "        \n",
    "        data = { \"player1Action\": player1Action\n",
    "               , \"player2Action\": \"Cooperate\" # Goody-two-shoes\n",
    "               , \"startingState\": self.state\n",
    "               }\n",
    "        \n",
    "        # Do a post to the server; get the payoffs.\n",
    "        response = requests.post(\"http://localhost:3000/play\", json=data).json()\n",
    "        \n",
    "        # Update the state.\n",
    "        self.state[\"totalPlayer1\"] += response[\"lastPlayer1Payoff\"]\n",
    "        self.state[\"totalPlayer2\"] += response[\"lastPlayer2Payoff\"]\n",
    "        \n",
    "        obs = response[\"lastPlayer1Payoff\"]\n",
    "        \n",
    "        # TODO: The reward should NOT be the amount of time in jail; it should\n",
    "        # the DIFFERENCE\n",
    "        \n",
    "        # reward = 10 - 6\n",
    "        # reward = 4 -> Good for me\n",
    "        \n",
    "        # reward = 1 - 19\n",
    "        # reward = -18 -> Bad for me.\n",
    "        reward = self.state[\"totalPlayer2\"] - self.state[\"totalPlayer1\"]\n",
    "        \n",
    "        return [ 0, reward, self.done, response ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3920a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = IteratedPDEnv(env_config = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfbd485",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(action=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e95c11",
   "metadata": {},
   "source": [
    "### Let's try training it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac47323",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(local_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc9311d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = PPOTrainer(env=IteratedPDEnv, config={\n",
    "    \"framework\": \"tf2\",\n",
    "    \"num_workers\": 1,\n",
    "    \"env_config\": {},\n",
    "    \"create_env_on_driver\": True\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb29a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94344076",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd4766",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
